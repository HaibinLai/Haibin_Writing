---
{"dg-publish":true,"permalink":"/CPP/CPP Pro5 Web/","created":"2024-09-09T14:05:54.835+08:00","updated":"2024-09-09T20:50:50.250+08:00"}
---


# **CS205·C/C++ Programming** 
# Project5 Report:  The beginning of Accelerated Computing

------------
PDF 版本：[Project 5](https://www.haibinlaiblog.top/wp-content/uploads/2024/09/Project5赖海斌.pdf)
Github: [https://github.com/HaibinLai/CS205-CPP-Programing-Project](https://github.com/HaibinLai/CS205-CPP-Programing-Project)
## 摘要

“这是一个令人惊叹的时代，因为我们正处于一场新的工业革命的开始，过去蒸汽机、电力、PC和互联网带来了信息革命，现在是人工智能。前所未有的是，我们正在同时经历两种转变：通用计算的结束和加速计算的开始。”

—— _Jensen Huang_

nvcc会不会对GPUKernel做优化？NCCL是如何让GPU互相通信的？复杂的递归DFS算法能不能用到并行的GPU程序中？怎么样对单个线程进行编程？GPU如何处理if分支语句？谁可能是下一代BLAS？在本次Project中，上面的问题都会得到探讨。

**关键词：CUDA；GPU；HPC；AI；**


## Part 1: 为什么需要加速计算

在本次Project中我们将探讨NVIDIA GPU在SGEMM中的运行情况与CUDA优化策略，同时查看在不同应用场景与价位下GPU的架构设计、技术细节与性能表现情况，通过这些学习，对加速计算给一个简单的概览。


## Part 2: B=aA+b实现

本次实现建立在Project4的实现的矩阵类上，增添了这么几个函数：

1. `a * A` 函数

GPU Kernel:
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909145934.png)


CPU host:
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909192323.png)


2.  `A’ + b`

GPU:
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909192649.png)

CPU hosts:
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909192704.png)

测试：
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909192804.png)


## Part 3: cuBLAS速度对比

1. 时间测量：

CUDA专门提供了测量时间的API函数: cudaEvent_t, cudaEventRecord(), cudaEventElapsedTime() 等函数，具体操作如下：
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909192841.png)

在CPU情况下，我们依旧使用project3中HPL的测量方法进行时间测量。
我们首先得出CPU BLAS SGEMM的计算时间：

| CPU                                      | N    |
| ---------------------------------------- | ---- |
| Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz | 4096 |

| MKL 单次 | MKL 1000次循环平均值 | OpenBLAS |
| ------ | -------------- | -------- |
| 0.118  | 0.5962         | 0.114    |
单位：秒


具体的测试分析我们已经在Project3中提到。我们这里是第一次对MKL的JIT进行测试，看来它还是取得了不错的效果。

我们接下来实现cuBLAS:
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909193009.png)


我们在RTX2080Ti上进行多次测试，求得`4096*4096`下的CUDA SGEMM时间约为78.06 ms，比CPU的BLAS有一定的提升。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909193056.png)


有趣的是，如果我们这里改进一下，把Matrix E这里加一个for循环：
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909193329.png)

按照道理它会耗时 `78*100 毫秒`，但是实验的结果是，它仅耗时约`1600ms`。我认为造成这样的主要原因是缓存仍停留在SM中，这使得CUDA可以重复利用数据。

诶，那用我们原始的plain方法，大概能多少呢？
`151ms`。虽然比CPU还好，但是跟cuBLAS还是差了一倍。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909193502.png)


去掉if，减少Thread divergence。
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909193525.png)


然后我们更换一下我们的blockDim。查看他们所需要的毫秒数。

| 16*16 | 32*32 | 48*48 | 64*64 |
| ----- | ----- | ----- | ----- |
| 137   | 133   | 114   | 144   |

这些是由于GPU的结构引起的。我们接下来由于有一个不错的范本，我们就可以跟着这个范本来研究如何加速cuBLAS。


## Part 4: 矩阵乘法加速分析

Github上有个人的GEMM算法写的挺好的，我对此进行分析和借鉴：

[cuda_sgemm/gemm.cu at master · njuhope/cuda_sgemm (github.com)](https://github.com/njuhope/cuda_sgemm/blob/master/gemm.cu)

1. grid 数和block数。
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909193854.png)


我们将GEMM进行分块，每一个grid分到`128*128`的小块，每一个Block分到256的块。然后，我们在核函数中对这`256*256`小块进行细分地操作。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909193923.png)

核函数：

2. 循环展开

跟在CPU一样，给每一个线程分配多个任务，可以让线程更加地连续，同时线程访问存储的速度也会因为连续而更快。

Reg的使用是PTX CUDA汇编的操作，通过使用寄存器，我们相当于是人肉NVCC的O3优化，将数据拉到SIMD更近的地方进行操作。

`#pragma unroll`是NVCC编译器独特的辅助编译方式，它可以帮助我们进一步将循环展开分配。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909194114.png)

3. share共享内存

在同一个warp内使用共享内存，这样的话计算的时候就可以分配任务，各个线程执行各个矩阵小块的工作。注意到GPU内的共享内存其实是非常的宝贵的，我们这里是smem并不能设置的太大。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909194147.png)


4. 连续数据

我们将共享内存中的数据导入到各自的reg中，这样可以提升我们的乘法计算速度。
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909194732.png)

在这样之后，我们的gemm在访存上就被很大地优化了。同时，我们的线程分配也让我们的计算效率基本上最大化。

我这里统计了一下身边可以计算的卡。然后对他们进行一一的测试。但是由于时间限制，我没有测算我们的自己的cuBLAS时间。

| 名称              | 类型         | 架构     | 发布日期            | CUDA 支持版本 | 功耗    | 价格        |
| --------------- | ---------- | ------ | --------------- | --------- | ----- | --------- |
| A100 80G        | AI、HPC数据中心 | Ampere | June 26th, 2021 | 12.4      | 300W  | 170000RMB |
| A100 40G        | AI、HPC数据中心 | Ampere | May 15th, 2020  | 12.4      | 300W  | 84000RMB  |
| V100            | AI、HPC数据中心 | Volta  | May 11th, 2017  | 12.1      |       | 43000RMB  |
| TITAN RTX       | 图形学、深度学习   | Turing | Dec 3rd, 2018   | 12.3      |       | 6000RMB   |
| RTX 2080Ti      | 游戏         | Turing | Oct 8th, 2018   | 12.4      | 250W  | 2600RMB   |
| RTX 3060 Laptop | 游戏         | Ampere | Feb 2nd, 2021   | 12.4      | <100W | 2200RMB   |
| GT 1030         | 游戏         | Pascal | May 17th, 2017  | 9.1       | 100W  | 248RMB    |
| Quadro P2000    | 图形学        | Fermi  | Dec 24th, 2010  | 2.1       | 80W   | 93RMB     |

| GPU             | cuBLAS花费时间 |
| --------------- | ---------- |
| RTX 3060 Laptop | 150.32     |
| A100 80G        | 18.09      |
| A100 40G        | 46.15      |
| V100            | 41.44      |
| RTX 2080Ti      | 79.80      |
| Quadro P2000    | 3603.50    |
| GT 1030         | 2353.65    |
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909194942.png)

Quadro 2000，淘宝87块钱的高级货，搭配E5-2666 v3神教
GT1030，入门级显卡：

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195006.png)

本次的Project，我想把时间更投入到对GPU的研究进展当中，所以我就没有再进行GEMM的具体计算和探究。


## Part 5: 论文赏析

没活了，给大伙表演一个论文解读吧。通过几篇论文，我们就能对GPU研究领域中的一个方向有所了解。接下来这篇会议论文是：GPU上MBE算法的加速。我们将看到GPU研究中的一个方向：**复杂算法加速**。(Accelerating Maximal Biclique Enumeration on GPUs [https://arxiv.org/pdf/2401.05039](https://arxiv.org/pdf/2401.05039))

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195043.png)

**摘要介绍：**

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195143.png)

MBE是当今很重要的算法，但是基本都是在CPU上实现。这篇文章里，我们将MBE算法引入到GPU。但是这里边我们遇到了3个主要问题：

1. **内存不足**。MBE图规模大，对内存需求高，GPU内存不够；

2. **线程分歧多**。MBE中有很多if分支，GPU在分支上会遇到Thread divergence，GPU的运算效率会下降；

3. **负载不均衡**。MBE中不同的搜索由于图形状不同，运算所需时间也不一样，导致有的线程跑的时间长，有的短。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195157.png)

针对这三个问题，本文提出了对应的3个方法：

1. 设计 node reuse approach降低内存用量。

2. 使用pro-active method剪枝，利用节点邻居数，减少线程分歧。

3. 设计load-aware 任务调度框架实现GPU warps内部线程负载均衡。

我们的效果：在A100 上运行速度比96核服务器快70.6倍。


**什么是MBE算法？**
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195239.png)

MBE算法用于在给定的图G(V,E) 中找到所有的最大完全二部子图maximal bicliques。在离散数学我们学到，二分图就是把一个图的顶点划分为两个不相交子集 ，使得每一条边都分别连接两个集合中的顶点，且集合内顶点不临接。


![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195259.png)

在一个图中寻找最大（点/边最多）完全二部子图是在生物信息学、数据挖掘等应用中的重要算法。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195314.png)



**为什么用GPU？**

CPU方面我们已经有很好的MBE算法了，但是随着这几年在深度学习、数据挖掘等在GPU上的应用的更多，大家开始希望有GPU上的MBE算法。同时，MBE在社交媒体推荐、基因表达、GNN信息聚合中应用的更多了，但是这些应用的数据量比以前更大，占用内存更多，CPU的计算速度开始逐渐跟不上了。

**以往有人研究过这个方面吗？有什么缺点？**
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195437.png)


有的，最早期的MBE设计出来后，人们就发现计算开销非常高。于是有人从算法设计上开始用剪枝等操作降低开销，但是他们都是在单核上跑的。后来有人设计了多核的，但是还是在CPU上。所以我们需要一个在GPU上的。（另外是，也有人设计了GPU上MBE算法，比如这篇文章，但是他们加速比最高只有38，那他们为什么没有我们快呢？原因就在下面）

**我们的三个困难具体是怎么样的？我们如何解决？**

首先是内存问题。直接从CPU搬运到GPU的算法需要大内存空间，同时它需要经常**分配内存**，这种动态内存分配对GPU运算来说也是一种性能损耗。

第二是现有MBE算法存在 irregular computation问题。我们知道假如GPU同时执行1000条加法指令，它会干的很好。但是MBE问题会要求GPU在同一个Warp内的不同线程都执行不一样的路径。而这会导致动态分支。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195451.png)

### thread divergence

这里我简单介绍一下thread divergence。我们的CPU是零售的话，我们GPU就是批发。而批发导致我们无法对个别线程很好地处理。比如我们在一个warp中有 `if(sum[x] < 100){ ... }`，如果我们是CPU，在计组上我们学过，我们会根据结果flush掉后面刚刚fetch的指令，或者运用分支预测技术选好可能会走下去的分支。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195501.png)
但是我们GPU如果在warp当中，某些数据选择肯定条件，某些选择否定条件，因为我们使用的是SIMD模块进行操作，做法跟CPU完全不同。
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195542.png)

在编程层面上，我们常常在核函数中写下这么一句话：

```
 int tid = blockIdx.x* blockDim.x + threadIdx.x;
```

但是，我们的tid其实是一种逻辑化的东西，我们认为线程都是独立连续的。但是在物理实现上，GPU内的线程是以32个线程为一组，编成一个线程束 warp。而GPU正是由这些线程束统一构成，每一个束基本上是执行命令的一个小单元。
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195904.png)


在早期Pascal架构中，面对if分支指令，一个warp会先跑一遍A,B，随后再跑另一边else的X，Y，所用时间是他们的总和，效果非常的铸币。为什么呢？因为一个warp只有一个PC，这个PC只会一次fetch一条指令，才导致了这个后果。

在Volta架构开始，线程开始变得精细化，每个线程都有一个自己的PC，这代表着每个线程可以执行自己想执行的指令了。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195922.png)
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195929.png)
虽然这听起来就像南科大任选课程任选专业一样非常的自由，但是在实际执行中我们仍然不能有部分线程执行A，同时有的线程执行X。主要原因是害怕A和X会互相影响。它这么做的目的是可以精细化管理线程。它的力量我们会在后面发现。

不过你这时可能要问了：诶，那这样我们好像执行的时间还比之前早期的用时还长啊！原本时间是A+B+X+Y+Z，现在变成A+X+B+Y+Z+Z了。这里要注意的是，GPU在执行时有访存时间约束。

这就提到GPU的另一个技术：**throughput**。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909195949.png)

假设我们现在的任务是光栅渲染。现在，我们使用GPU中的warp来渲染。我们将取出数据，EX，然后MEM，这里边，我们会花很多时间进行访存，然后再存储。此时GPU的ALU不就没事干了，但是闲着也是闲着，GPU就会像上下文切换一样，换一批程序进行处理。这样，当比如说我们的着色器程序遇到内存读取操作时，如访问纹理，因为32个threads执行的是相同的程序，所以它们会同时遇到该操作，内存读取意味着该线程组将会阻塞（stall），全部等待内存读取的结果。为了降低延迟，GPU的warp调度器会将当前阻塞的warp换出，用另一组包含32个线程的warp来代替执行。换出操作跟单核的任务调度一样的快，因为在换入换出时，每个线程的数据都没有被触碰，每个线程都有它自己的寄存器，每个warp都负责记录它执行到了哪条指令。换入一个新的warp，不过是将GPU 的shader cores切换到另一组线程上继续执行，除此之外没有其他额外的开销。通过这样精细化线程，我们发现我们可以更好地利用好访存的时间进行别的运算，降低运算总体时间。但是，在这种技术下的运行时间仍然是大于直接运算的时间的。

因此，分支是并行计算里一个比较可怕的事情。因为分支，本来可以一起add的指令，现在只能你add我sub，这样的性能下降是我们在把一般的算法用到CUDA中必须要考虑的问题。

第三个问题是负载均衡问题。可能有的子图非常大，给一个线程算就太慢了。而GPU每次就必须等最慢的那个子图算完才能输出，此时其他线程都在围观，这是非常难受的。

以往的GPU在图论方面的算法有maximal clique enumeration, graph pattern mining，优化方法有data graph partitioning, two-level parallelism, adaptive buffering, hybird order。但是他们都只适用于小点数的图，面对大点数的MBE无能为力。

但是我们就比较牛。第一，面对内存问题，我们将递归变回for+栈（递归本质），同时重复利用root node的Memory而不是自己copy新的一份。第二面对分支问题，我们使用local neighborhood size来减少搜索空间，从而减少if的次数。第三面对负载问题，我们会根据subtree的具体大小来动态分配线程任务。


我们用的操作是使用 fast CUDA primitives 线程束级原语来实现我们的GPU上MBE（后称GMBE）算法。"线程束级原语" 指的是 CUDA 中针对线程束（thread warp）级别的操作或功能。线程束是 CUDA 中最小的并行执行单元，通常包含 32 个线程。线程束级原语允许开发者直接操作线程束，以实现更细粒度的控制和优化。

我们拿下面这个例子进行介绍，在一个带选择分支的语句中，我们先算出这个线程的tid（thread id），然后根据线程id进行判断。但是正如我们刚才所说，这会导致一个warp内32个线程，有16个走左边，16个走右边的情况发生。而我们希望的是，一个warp 32线程走左边，另一个warp 32线程走右边。

``` CPP
__global__ void mathKernel1(float *c)
{

int tid = blockIdx.x* blockDim.x + threadIdx.x;

float a = 0.0;
float b = 0.0;

if (tid % 2 == 0)
{
 a = 100.0f;
}

else
{
  b = 200.0f;
}

c[tid] = a + b;

}
```

那么我们就可以这样写我们的代码：在判断时加上warpSize，第一个线程束内的线程编号tid从0到31，tid/warpSize都等于0，那么就都执行if语句。第二个线程束内的线程编号tid从32到63，tid/warpSize都等于1，执行else语句。

这样，我们的线程束内就没有了分支，提高了我们的效率。 线程束级原语的设计是目前学界感兴趣的一项研究项目。文章[7]探讨了不同的线程束级原语的设计及对效率的影响。文章[8]使用了原语来实施最小生成树算法（如右图）。文章[9]利用了原语中的Scan（类似于MPI Scan，将各个线程的结果发至一个线程），实现了GPU上的高效快速排序和稀疏矩阵乘法，并将其应用到图形浅水流体模拟上。文章[16]则是一篇古老的文章，它是最早将渲染算法用更精细的原语实现的文章。

原语中有很多精细的操作。比如当warp中的线程需要执行比数据交换原语所提供的更复杂的通信或选择操作时，可以使用该__syncwarp()原语来同步warp中的线程。它类似于__syncthreads()原语（同步线程块中的所有线程），但粒度更细。

`void __syncwarp(unsigned mask=FULL_MASK);`

`__syncwarp()` 原语使正在执行的线程等待，直到指定的所有线程mask都已执行了`__syncwarp()`（具有相同的mask），然后才恢复执行。它还提供了一个 内存屏障， 以允许线程在调用原语之前和之后通过内存进行通信。

每个“同步数据交换”原语在线程束中的一组线程之间执行集合操作。例如，表2显示了其中的三个。每个线程在同一warp中从一个线程调用`__shfl_sync()`或` __shfl_down_sync()`接收数据，并且每个调用的线程都`__ballot_sync()`接收一个位掩码，该掩码表示warp中所有传递谓词参数真值的线程。

```CPP
int __shfl_sync(unsigned mask, int val, int src_line, int width=warpSize);

int __shfl_down_sync(unsigned mask, int var, unsigned detla,
                     int width=warpSize);

int __ballot_sync(unsigned mask, int predicate);
```

参与调用每个原语的线程集使用32位掩码指定，这是这些原语的第一个参数。必须使所有参与线程同步，以使选择操作正常工作。因此，如果这些原语尚未同步，则它们首先将同步线程。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909202941.png)


```CPP
__global__ void mathKernel2(float *c)
{

  int tid = blockIdx.x* blockDim.x + threadIdx.x;
  float a = 0.0;
  float b = 0.0;
if ((tid/warpSize) % 2 == 0)
{
   a = 100.0f;
}
else
{
  b = 200.0f;
}

  c[tid] = a + b;
}
```


另外，我也看到可以使用下面的命令查看CUDA运行时的branch efficiency：

`nvprof _--metrics branch_efficiency ./divergence_`

这个branch efficiency是由下面这个计算公式得来的：
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909203208.png)
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909203221.png)


有趣的是，文章中还提到，这种分支效率有时候会被nvcc编译器优化改进。也就是说，我们的nvcc还是会做一些优化的，只不过不是CPU上的优化，而是在GPU上的。

接下来就是论文里的具体操作了。原始的方法是使用递归进行操作，然后每一个线程负责不同的节点底下不同的分支，在DFS下找到新的节点，判断构建的二部图是否可以继续扩大。

那么我们首先进行一下内存的分析。首先按照这样的算法的操作，我们根据测试案例，给到每一个SM的子图大概需要3.67GB。而如果想利用上A100内的108个SM，我们需要397GB。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909203259.png)

那么我们自然会想到，诶既然你给每一个SM都来一个子图，你为什么不造一个更大的图，然后几个SM一起共用，这样子不就节省了一些内存吗？确实，他们就是这么做的。请看算法。

如图所示，将递归更改成栈之后，我们的node buffer就可以继续利用下去，从而使得GPU内的资源就可以重复利用。虽然这样存储单个子树的大小提高，但是整体的大小变小。在选用的数据集上，因为每个过程只需要（3×13,601 + 2×53,915）×sizeof(int) = 595 KB。与3.1节中讨论的简单实现需要13,601×（13,601+53,915）×sizeof(int)= 3.67 GB相比，这种节点重用方法极大节省了内存空间。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204257.png)

### Pruning

下一个优化是剪枝。由于这里跟文章的算法紧密相关，我就简单介绍一下。我们这个GMBE算法里最大的if分支是右图的第10-13行。而在这之中作者通过证明发现，我们将一个顶点v∈V的local neighbors定义为v连接的其他节点。比如图5中的v4，连着u4,u5两个点，local neighbors数量为2。V3连着u1,u2,u4，数量为3。接着，我们发现如果跳出遍历子节点到别的节点，它们当中的点的local neighbors大小不改变，我们就可以不用去检验大小不改变的节点。这样的剪枝方法具有较低的线程散度，因为不同的线程总是检查同一候选集中的元素。


![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204313.png)

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204332.png)

### PT

面对下一个负载均衡的问题，研究人员使用了一个叫 persistent thread 持久线程的东西。什么是持久性线程？

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204354.png)

这是一个更加底层并且提升效率的线程设计。文章[5]重点分析了这个设计。GPU的硬件和编程风格的设计使得它们严重依赖于单指令多线程（SIMT）和单程序多数据（SPMD）编程范例。这两种范式都**虚拟化了多层次的底层硬件**，从实际的硬件操作中抽象出软件程序员的视图。物理SIMD流多处理器（SM）的每个通道都被虚拟化成更大批量的线程，这些线程是SIMD通道宽度的倍数，称为扭曲或波前（SIMT处理）。就像在SIMD处理中一样，每个warp都以锁步的方式操作，并执行相同的指令，在硬件处理器上进行时间多路复用。多个线程被组合在一起，形成一个更高的抽象，称为线程块blocks，每个块内的线程允许在运行时通过L1缓存/共享内存或寄存器进行通信和共享数据。该过程被进一步虚拟化，多个线程块被同时调度到每个SM上，每个块在不同数据上的不同程序实例上独立运行（SPMD处理）。

这种编程风格（我们将称之为“non-PT”）迫使开发人员将工作单元抽象到虚拟线程中。由于块的数量取决于工作单元的数量，在大多数情况下，在硬件上运行比内核启动时启动的块多几百或数千个块。在传统的编程风格中，这些额外的块在运行时被调度。块的切换完全由一个硬件调度器来管理，而**程序员则没有办法影响如何将块调度到SM上**。因此，虽然这些抽象通过为来自各种应用程序领域的开发人员提供了一个低的程序进入模型，但**它阻碍了经验丰富的程序员工作已经难以并行化的高度不规则的工作负载**。这暴露了当前SPMD编程风格的一个重大限制，它既不保证顺序、位置和时间，也不明确允许开发人员不影响上述三个参数。

另外一些可怕的性质，比如，1.主从设备导致GPU只能听从CPU发送具体代码，2.运行时决策使得我们不能保证将在何时何地调度块。3.块状态设计让GPU 的一个新的块被映射到一个特定的SM时，该SM上的旧状态（寄存器和共享内存）会被认为是过时的，不允许块之间的任何通信，即使在同一SM上运行时也是如此。4.块间通信的唯一机制是全局内存，块作为独立内容，这样的通信可能性能不足。5.生产消费结构，内核只能在运行到完成时生成数据。在这个内核产生的GPU上的数据需要另一个内核。6.内核独立单一，即内核不能调用自身的另一个副本（递归），不能生成其他内核，或者动态添加更多块。在调用之间存在数据重用的情况下，这种成本尤其昂贵。

为了规避这些限制，PT诞生了，它可以涉及较低层次的抽象化，通过直接控制调度来提升性能。全文中最直白地解释持久化线程的便是下面这幅图：

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204414.png)

在这幅图片里，假设我们的GPU一共有4个SM来执行对Lenna图像的处理。如果是nonPT的普通线程，GPU内的硬件调度器就会启动16个block来执行处理操作，而这些block会在物理层面上对应到相应的SM上（SM0,SM1,SM2,SM3），但是，我们可以看到这样的操作就有点不是很好，1.调度器要重新分配线程和数据；2.如果是对图像变暗变亮，那还好，但是如果是卷积或者跟相邻块要共用的操作，那我们要花一部分时间重新搬运回数据。

但是我们看右边的PT线程，他们从始至终只有4个，对应4个SM。首先从内存访问上来说，我们可以更好地安排存储，Cache在导入n-way数据后，我们的SM访问时的Cache Hit Rate就可以更高（又是你，计组）。同时，数据在像素跨垂直块边界共享时进行重用，这对卷积什么的操作也挺不错的。

因此，编程的持久线程风格改变了虚拟软件线程的生命周期的概念，使它们更接近物理硬件线程的执行寿命，也就是说，让程序员能够控制线程在内核的持续时间。

那么，PT线程有什么样的功用呢？文章[5]给了4个例子：

1. CPU-GPU同步
2. 负载均衡（也是我们的前面文章用到的）
3. 本地化生产者-消费者
4. 全局同步

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204440.png)

### **CPU-GPU同步**

对于第一个功用，由于CPU（主机）和GPU（设备）作为主设备和从设备耦合在一起，因此设备缺乏向自己提交工作的能力。除了内核中内置的功能之外，GPU还依赖于主机来发出所有的数据移动、执行和同步命令。如果生产者内核为消费者内核在运行时处理生成可变数量的项目，主机必须发出回读消息，确定消耗中间数据所需的块数，然后启动新内核。读取备份有显著的开销，特别是在主机和设备不共享同一内存空间或不在同一芯片上的系统中。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204500.png)

但是，在有持久性线程后，数据一旦从CPU来到GPU-kP（生产者），之后就能一直计算到GPU-kC，数据不会因为线程结束而自动消失，GPU会保持运算一直到需要再次访问数据的时刻。这样，我们就节省了重新向CPU申请大量数据的时间。

这里涉及到一个生产者消费者模型的概念。我推测它跟OS的生成者消费者模型是差不多的。在我们的CPU多线程中，我们将程序分成生产者和消费者。生产者生产数据，消费者使用数据。
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204514.png)

如果生产者生产数据的速度很快，而消费者消费数据的速度很慢，那么生产者就必须等待消费者消费完了数据才能够继续生产数据，同理如果消费者的速度大于生产者那么消费者就会经常处理等待状态，所以为了达到生产者和消费者生产数据和消费数据之间的平衡，那么就需要一个缓冲区buffer用来存储生产者生产的数据。

生产者消费者模式就是通过一个仓库buffer来解决生产者和消费者的速度不匹配问题。生产者和消费者彼此之间不直接通讯，他们通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。这个阻塞队列就是用来给生产者和消费者解耦的。

这样，我们就可以增加生产者线程或者消费者线程，而不会对整个程序的正确性产生影响，同时我们也可以根据不同硬件访问速度的不同动态地控制两者的比例。

在GPU上，我们通过将生产者和消费者PT化，让他们持久地运输数据或计算数据，就可以避免开启线程以及重新搬运数据的时间。


### **负载均衡**

文章[12]针对光线射线追踪，使用了PT进行加速。

在文章中，PT的主要作用是通过保持一些长时间运行的射线线程，避免了频繁创建和销毁线程的开销，从而提高了SIMD的效率和利用率。通过让每条射线处理时所有的遍历都保持在一个块中，绕过硬件调度器，从而可以提高线程对数据的重复利用。而在短时间内完成渲染的PT，可以接着拿到下一个分配任务。如果我们不使用PT，我们就会重开一个线程，刷新掉缓存里的数据，开销就更大了。

因此，PT对于不规则的数据处理，比如树、图，是比较有效的。
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204728.png)


**本地化生产者-消费者**

在我们了解了CPU内的生产者消费者模型后，这个标题我们自然也很好理解了。
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204757.png)

对于一个生产者消费者，如果我们不使用PT，我们的Kernel之间想实现这个模型，就只能是一个生产者模型，一个消费者模型。他们通过GPU L2 Cache或者DRAM通信。这样的成本就非常大了。但是，如果我们现在只使用一个Kernel，它经历两个阶段：阶段A是生产者，将数据拉到GPU并尝试填满内存；如果内存满了，Kernel进入阶段B：消费者，Kernel开始搬运数据试图清空内存。此时Kernel由于是在PT情况下执行，这些内存始终是在离Kernel物理执行的SM处最近的地方，因此此时访存的成本就可以大大降低了。

### **全局同步**

GPU为在同一个块中的线程同步提供了硬件支持，但GPU对在同一个SM中的不同block乃至整个GPU中，没有提供同步。那么我们同样是可以使用一个大PT来解决问题的。我们将不同的Kernel合并成一个Kernel-X’，当每个blocks完成一个计算时，它就开始处理下一个阶段的blocks。这个阶段就是对块进行全局同步的阶段。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204843.png)
![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204907.png)


所以我们可以看到，PT作为一个持久化的线程，让我们更好地操作我们的GPU。它的语法格式也还比较简单，具体类似如下：

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204851.png)

CPU在lunch线程后，CUDA会接收说哪个线程现在处于启动状态（有点像CPU的线程启动模式），然后CPU说线程启动，CUDA就开始计算，CPU会在waitforCUDA那边等待GPU线程完成的信号。

现在我们回到论文，通过在PT上进行改进，我们就可以对特定的搜索给定特定的线程，从而加速搜索的速度。

随后文章就到了实施细节和测试环节。实施环节提到了写算法时要特地注意的部分，测试则针对不同的数据集，不同的GPU，多GPU进行了测试，并比较了三个优化产生的效果。这里我们就不再详细查看。但是总结一下的话，这篇文章里我们学到了下面3个技术以及研究方向：

1. 线程束级原语——更精细化的核函数设计

2. PT持久线程——核函数间更负载均衡

3. 内存复用——更好的内存设计


![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909204930.png)


从这篇文章中我们可以看到，1.内存是我们开始加速计算的一个起点，解决不了内存问题，加速计算就无从谈起。2.线程计算的精细化与复杂化是目前加速计算研究的一个方向，如何让线程计算更复杂的算法，让原本单核运行的算法多核化，才能让加速计算的应用方向更加广泛。

在GPU上复杂算法的实现是目前GPU研究中的一个重要方向。在矩阵计算之外，我们还有很多算法支撑着我们的生活，而要想更快，我们的移植是无法阻挡的。在这之中，想实现一个好的移植算法，我们就要操控好线程，进行精细化的设计。


“

有个周末我带女儿 Madison 去书店，然后就看到了这本书 OpenGL手册，定义了硅谷图形的计算机图形处理方式。一本 68 美元，我带了几百块钱，买了三本。

我把书带回办公室，对大家说：「我找到了咱们的未来。」我把三本书分发下去传阅，中间有大幅的折叠插页，这个插页就是OpenGL流水线计算机图形处理流水线。我把它交给了与我共同创办公司的那些天才手中。

我们以前所未有的方式实现了OpenGL流水线，构建出了世界从未见过的东西。其中有很多经验教训。对我们公司来说，那一刻给了我们极大的信心：即使对所做的事情一无所知，也能成功创造出未来。

**现在这就是我对任何事情的态度。当有人跟我说我没听过的事情，或者听说过但不懂原理，我的想法总是：能有多难呢？可能看本书就搞定了，可能找一篇论文就能搞清楚原理。**

我确实花了很多时间阅读论文，这是真的。当然，你不能照搬别人的做法，指望会有不同的结果。但你可以了解某件事情的实现原理，然后回归问题的本质，扪心自问：基于现有的条件、动机、手段和工具，以及一切如今的变革，我会怎么去重做这件事？我会如何重新发明它？我会如何设计它？

如果今天造一辆车，我会沿用过去的方式吗？如果今天让我创造一台计算机，我会采用怎样的方式？如果今天让我来编写软件呢？

这么想有道理吗？即使是今天的公司，我也经常回归本质，从头思考。这是因为世界已经变了。**过去编写软件的方式是单一的，是为超级计算机设计的，但现在软件架构已经解耦等等。我们今天思考软件、计算机的方式一直在改变。经常促使公司和自己回归问题本质，会创造出大量的机会。**

”




## **GPU通信**

看完这个方向后，我们再往一个系统一点的方向看看：**GPU通信**。虽然我们在Project4有所了解，但是我们可以再深入了解一下。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909205028.png)

我们从这篇2020年的综述性的文章开始，它评估了现代GPU通信方法：PCIe,NVLink,NV-SLI,NVSwithc, GPUDirect。

PCIe(Peripheral-Component-Interconnect-Express-Bus)是高速串行计算机扩展总线的标准，我们在Project4里详细介绍了PCIe与GPUDirect技术的点滴。然而，这对于需要大量数据的GPU来说，还是太慢了。传统的基于PCIe的CPU-GPU节点连接是一个树结构，CPU与GPU之间使用PCIe交换机连接，CPU内部使用QPI总线连接。但是，这意味着当GPU想跨树进行通信时，就得经过CPU。

![](https://blog-1327458544.cos.ap-guangzhou.myqcloud.com/CPP5/20240909205045.png)
